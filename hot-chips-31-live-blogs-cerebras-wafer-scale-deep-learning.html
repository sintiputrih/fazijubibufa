<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Cerebras' 1.2 Trillion Transistor Deep Learning Processor -</title><meta name=robots content="index,follow,noarchive"><meta name=description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology.

08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM

08:51PM EDT - TSMC 16nm
08:51PM EDT - 215mm x 215mm - 8.5 inches per side"><meta name=author content="Reinaldo Massengill"><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/app.css><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/an-old-hope.min.css><script defer src=https://assets.cdnweb.info/hugo/paper/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=./theme.png><link rel=icon href=./favicon.ico><link rel=apple-touch-icon href=./apple-touch-icon.png><meta name=generator content="Hugo 0.98.0"><meta property="og:title" content="Cerebras' 1.2 Trillion Transistor Deep Learning Processor"><meta property="og:description" content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta property="og:type" content="article"><meta property="og:url" content="/hot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-03-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-15T00:00:00+00:00"><meta itemprop=name content="Cerebras' 1.2 Trillion Transistor Deep Learning Processor"><meta itemprop=description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta itemprop=datePublished content="2024-03-15T00:00:00+00:00"><meta itemprop=dateModified content="2024-03-15T00:00:00+00:00"><meta itemprop=wordCount content="973"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Cerebras' 1.2 Trillion Transistor Deep Learning Processor"><meta name=twitter:description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"></head><body class=not-ready data-menu=true><header class=header><p class=logo><a class=site-name href=./index.html>BlogPaper</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=menu><a href=./sitemap.xml>Sitemap</a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>Mar 15, 2024</time>
<span>Reinaldo Massengill</span></p><h1>Cerebras' 1.2 Trillion Transistor Deep Learning Processor</h1></header><section class=post-content><p><a href=# id=post0819204952><span class=lb_time>08:49PM EDT</span></a> - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology.</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175010_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205102><span class=lb_time>08:51PM EDT</span></a> - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175016_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205119><span class=lb_time>08:51PM EDT</span></a> - TSMC 16nm</p><p><a href=# id=post0819205137><span class=lb_time>08:51PM EDT</span></a> - 215mm x 215mm - 8.5 inches per side</p><p><a href=# id=post0819205152><span class=lb_time>08:51PM EDT</span></a> - 56 times larger than the largest GPU today</p><p><a href=# id=post0819205227><span class=lb_time>08:52PM EDT</span></a> - Built for Deep Learning</p><p><a href=# id=post0819205231><span class=lb_time>08:52PM EDT</span></a> - DL training is hard (ed: this is an understatement)</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175210_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205255><span class=lb_time>08:52PM EDT</span></a> - Peta-to-exa scale compute range</p><p><a href=# id=post0819205302><span class=lb_time>08:53PM EDT</span></a> - The shape of the problem is difficult to scale</p><p><a href=# id=post0819205311><span class=lb_time>08:53PM EDT</span></a> - Fine grain has a lot of parallelism</p><p><a href=# id=post0819205317><span class=lb_time>08:53PM EDT</span></a> - Coarse grain is inherently serial</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175236_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205327><span class=lb_time>08:53PM EDT</span></a> - Training is the process of applying small changes, serially</p><p><a href=# id=post0819205337><span class=lb_time>08:53PM EDT</span></a> - Size and shape of the problem makes training NN really hard</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175342_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205358><span class=lb_time>08:53PM EDT</span></a> - Today we have dense vector compute</p><p><a href=# id=post0819205416><span class=lb_time>08:54PM EDT</span></a> - For Coarse Grained, require high speed interconnect to run mutliple instances. Still limited</p><p><a href=# id=post0819205422><span class=lb_time>08:54PM EDT</span></a> - Scaling is limited and costly</p><p><a href=# id=post0819205455><span class=lb_time>08:54PM EDT</span></a> - Specialized accelerators are the answer</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175439_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205504><span class=lb_time>08:55PM EDT</span></a> - NN: what is the right architecture</p><p><a href=# id=post0819205528><span class=lb_time>08:55PM EDT</span></a> - Need a core to be optimized for NN primitives</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175510_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205532><span class=lb_time>08:55PM EDT</span></a> - Need a programmable NN core</p><p><a href=# id=post0819205538><span class=lb_time>08:55PM EDT</span></a> - Needs to do sparse compute fast</p><p><a href=# id=post0819205542><span class=lb_time>08:55PM EDT</span></a> - Needs fast local memory</p><p><a href=# id=post0819205552><span class=lb_time>08:55PM EDT</span></a> - All of the cores should be connected with a fast interconnect</p><p><a href=# id=post0819205620><span class=lb_time>08:56PM EDT</span></a> - Cerebras uses flexible cores. Flexible general ops for control processing</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175559_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205629><span class=lb_time>08:56PM EDT</span></a> - Core should handle tensor operations very efficiency</p><p><a href=# id=post0819205637><span class=lb_time>08:56PM EDT</span></a> - Forms the bulk fo the compute in any neural network</p><p><a href=# id=post0819205645><span class=lb_time>08:56PM EDT</span></a> - Tensors as first class operands</p><p><a href=# id=post0819205720><span class=lb_time>08:57PM EDT</span></a> - fmac native op</p><p><a href=# id=post0819205758><span class=lb_time>08:57PM EDT</span></a> - NN naturally creates sparse networks</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175738_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205810><span class=lb_time>08:58PM EDT</span></a> - The core has native sparse processing in the hardware with dataflow scheduling</p><p><a href=# id=post0819205815><span class=lb_time>08:58PM EDT</span></a> - All the compute is triggered by the data</p><p><a href=# id=post0819205823><span class=lb_time>08:58PM EDT</span></a> - Filters all the sparse zeros, and filters the work</p><p><a href=# id=post0819205838><span class=lb_time>08:58PM EDT</span></a> - saves the power and energy, and get performance and acceleration by moving onto the next useful work</p><p><a href=# id=post0819205848><span class=lb_time>08:58PM EDT</span></a> - Enabled because arch has fine-grained execution datapaths</p><p><a href=# id=post0819205855><span class=lb_time>08:58PM EDT</span></a> - Many small cores with independent instructions</p><p><a href=# id=post0819205901><span class=lb_time>08:59PM EDT</span></a> - Allows for very non-uniform work</p><p><a href=# id=post0819205905><span class=lb_time>08:59PM EDT</span></a> - Next is memory</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175910_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205934><span class=lb_time>08:59PM EDT</span></a> - Traditional memory architectures are not optimized for DL</p><p><a href=# id=post0819205946><span class=lb_time>08:59PM EDT</span></a> - Traditional memory requires high data reuse for performane</p><p><a href=# id=post0819210004><span class=lb_time>09:00PM EDT</span></a> - Normal matrix multiply has low end data reuse</p><p><a href=# id=post0819210028><span class=lb_time>09:00PM EDT</span></a> - Translating Mat*Vec into Mat*Mat, but changes the training dynamics</p><p><a href=# id=post0819210044><span class=lb_time>09:00PM EDT</span></a> - Cerebras has high-perf, fully distributed on-chip SRAM next to the cores</p><p><a href=# id=post0819210106><span class=lb_time>09:01PM EDT</span></a> - Getting orders of magnitude higher bandwidth</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180051_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210120><span class=lb_time>09:01PM EDT</span></a> - ML can be done the way it wants to be done</p><p><a href=# id=post0819210141><span class=lb_time>09:01PM EDT</span></a> - High bandwidth, low latency interconnect</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180126_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210147><span class=lb_time>09:01PM EDT</span></a> - fast and fully configurable fabric</p><p><a href=# id=post0819210158><span class=lb_time>09:01PM EDT</span></a> - all hw based communication avoicd sw overhead</p><p><a href=# id=post0819210205><span class=lb_time>09:02PM EDT</span></a> - 2D mesh topology</p><p><a href=# id=post0819210218><span class=lb_time>09:02PM EDT</span></a> - higher utlization and efficiency than global topologies</p><p><a href=# id=post0819210248><span class=lb_time>09:02PM EDT</span></a> - Need more than a single die</p><p><a href=# id=post0819210254><span class=lb_time>09:02PM EDT</span></a> - Solition is a wafer scale</p><p><a href=# id=post0819210315><span class=lb_time>09:03PM EDT</span></a> - Build Big chips</p><p><a href=# id=post0819210321><span class=lb_time>09:03PM EDT</span></a> - Cluster scale perf on a single chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180258_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210330><span class=lb_time>09:03PM EDT</span></a> - GB of fast memory (SRAM) 1 clock cycle from the core</p><p><a href=# id=post0819210336><span class=lb_time>09:03PM EDT</span></a> - That's impossible with off-chip memory</p><p><a href=# id=post0819210343><span class=lb_time>09:03PM EDT</span></a> - Full on-chip interconnect fabric</p><p><a href=# id=post0819210356><span class=lb_time>09:03PM EDT</span></a> - Model parallel, linear performance scaling</p><p><a href=# id=post0819210406><span class=lb_time>09:04PM EDT</span></a> - Map the entire neural network onto the chip at once</p><p><a href=# id=post0819210418><span class=lb_time>09:04PM EDT</span></a> - One instance of NN, don't have to increase batch size to get cluster scale perf</p><p><a href=# id=post0819210425><span class=lb_time>09:04PM EDT</span></a> - Vastly lower power and less space</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180435_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210457><span class=lb_time>09:04PM EDT</span></a> - Can use TensorFlow and PyTorch</p><p><a href=# id=post0819210512><span class=lb_time>09:05PM EDT</span></a> - Performs placing and routing to map neural network layers to fabric</p><p><a href=# id=post0819210522><span class=lb_time>09:05PM EDT</span></a> - Entire wafer operates on the single neural network</p><p><a href=# id=post0819210529><span class=lb_time>09:05PM EDT</span></a> - Challenges of wafer scale</p><p><a href=# id=post0819210557><span class=lb_time>09:05PM EDT</span></a> - Need cross-die connectivity, yield, thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180534_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180630_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210655><span class=lb_time>09:06PM EDT</span></a> - Scribe line separates the die. On top of the scribe line, create wires</p><p><a href=# id=post0819210702><span class=lb_time>09:07PM EDT</span></a> - Extends 2D mesh fabric across all die</p><p><a href=# id=post0819210711><span class=lb_time>09:07PM EDT</span></a> - Same connectivity between cores and between die</p><p><a href=# id=post0819210732><span class=lb_time>09:07PM EDT</span></a> - More efficient than off-chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180719_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210747><span class=lb_time>09:07PM EDT</span></a> - Full BW at the die level</p><p><a href=# id=post0819210810><span class=lb_time>09:08PM EDT</span></a> - Redundancy helps yield</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180753_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210831><span class=lb_time>09:08PM EDT</span></a> - Redundant cores and redundant fabric links</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180816_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210848><span class=lb_time>09:08PM EDT</span></a> - Reconnect the fabric with links</p><p><a href=# id=post0819210855><span class=lb_time>09:08PM EDT</span></a> - Drive yields high</p><p><a href=# id=post0819210902><span class=lb_time>09:09PM EDT</span></a> - Transparent to software</p><p><a href=# id=post0819210924><span class=lb_time>09:09PM EDT</span></a> - Thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180909_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210935><span class=lb_time>09:09PM EDT</span></a> - Normal tech, too much mechanical stress via thermal expansion</p><p><a href=# id=post0819210939><span class=lb_time>09:09PM EDT</span></a> - Custom connector developed</p><p><a href=# id=post0819210948><span class=lb_time>09:09PM EDT</span></a> - Connector absorbs the variation in thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181005_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211026><span class=lb_time>09:10PM EDT</span></a> - All components need to be held with precise alignment - custom packaging tools</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181031_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211049><span class=lb_time>09:10PM EDT</span></a> - Power and Cooling</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181056_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211117><span class=lb_time>09:11PM EDT</span></a> - Power planes don't work - isn't enough copper in the PCB to do it that way</p><p><a href=# id=post0819211128><span class=lb_time>09:11PM EDT</span></a> - Heat density too high for direct air cooling</p><p><a href=# id=post0819211206><span class=lb_time>09:12PM EDT</span></a> - Bring current perpendicular to the wafer. Water cooled perpendicular too</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181142_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181224_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211406><span class=lb_time>09:14PM EDT</span></a> - Q&A Time</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181341_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211445><span class=lb_time>09:14PM EDT</span></a> - Q and A</p><p><a href=# id=post0819211458><span class=lb_time>09:14PM EDT</span></a> - Already in use? Yes</p><p><a href=# id=post0819211515><span class=lb_time>09:15PM EDT</span></a> - Can you make a round chip? Square is more convenient</p><p><a href=# id=post0819211558><span class=lb_time>09:15PM EDT</span></a> - Yield? Mature processes are quite good and uniform</p><p><a href=# id=post0819211658><span class=lb_time>09:16PM EDT</span></a> - Does it cost less than a house? Everything is amortised across the wafer</p><p><a href=# id=post0819211722><span class=lb_time>09:17PM EDT</span></a> - Regular processor for housekeeping? They can all do it</p><p><a href=# id=post0819211741><span class=lb_time>09:17PM EDT</span></a> - Is it fully synchronous? No</p><p><a href=# id=post0819212001><span class=lb_time>09:20PM EDT</span></a> - Clock rate? Not disclosed</p><p><a href=# id=post0819212042><span class=lb_time>09:20PM EDT</span></a> - That's a wrap. Next is Habana</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH51g5RxZqGnpGKwqbXPrGRsaV2htrexjJujqJ%2BjYrCmvsSbqZqrXayup7HRZqqcmZyaeqWxxKlkpZ2Rp7uqusY%3D</p></section><nav class=post-nav><a class=prev href=./136377-adrienne-bailon-net-worth-age-children-husband-height-career-profiles.html><span>←</span><span>Adrienne Bailon's net worth, age, children, husband, height, career, profiles</span></a>
<a class=next href=./liz-macgill-resigns-all-on-husband-leon-francis-szeptycki-salary-net-worth-comments-children-and-more-article-105868475.html><span>Liz Magill Resigns: All On Husband Leon Francis Szeptycki, Salary, Net Worth, Comments, Children And</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2024 <a href=./></a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>